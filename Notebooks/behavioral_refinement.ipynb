{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ§  Behavioral Refinement\n",
                "\n",
                "In this step, we're sharpening our focus. Now that we have our initial habitual metrics, we'll apply more stringent classification thresholds to identify the 'Strong Mirrored' stations. We'll also normalize our scores to provide a standardized view of behavioral patterns across the entire network."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1. Setup and Data Loading\n",
                "We load our habitual metrics and prepare for the refinement process. This file is the output from our previous habitual analysis."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from pathlib import Path"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded 3,489 rows from habitual_metrics.csv\n"
                    ]
                }
            ],
            "source": [
                "DATA_DIR = Path(\"../data/processed\")\n",
                "input_path = DATA_DIR / \"habitual_metrics.csv\"\n",
                "output_path = DATA_DIR / \"refined_behavioral_scores.csv\"\n",
                "\n",
                "if not input_path.exists():\n",
                "    raise FileNotFoundError(\"\\u274c habitual_metrics.csv not found. Run habitual_analysis first.\")\n",
                "\n",
                "df = pd.read_csv(input_path)\n",
                "print(f\"Loaded {len(df):,} rows from habitual_metrics.csv\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2. Tightening the Mirror Classification\n",
                "To ensure we're targeting the most high-potential stations, we're 'raising the bar'. By increasing our routine score (RS) thresholds, we filter out stations that exhibit more leisure-heavy or incidental usage, focusing on those that truly mirror member-like commuter behavior."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "def classify_mirror(rs):\n",
                "    if rs >= 0.50:\n",
                "        return \"Strong Mirror\"\n",
                "    elif rs >= 0.40:\n",
                "        return \"Moderate Mirror\"\n",
                "    elif rs >= 0.30:\n",
                "        return \"Weak Mirror\"\n",
                "    else:\n",
                "        return \"Reject\"\n",
                "\n",
                "df['mirror_verdict'] = df['routine_score'].apply(classify_mirror)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3. Score Normalization and Export\n",
                "Lastly, we apply Min-Max scaling to our routine scores. This provides a clear, 0-to-1 scale that makes it easier to compare stations and prioritize our marketing efforts. The final refined dataset is saved for visualization and final reporting."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--------------------------------------------------\n",
                        "âœ… SUCCESS: Refined scores saved to ..\\data\\processed\\refined_behavioral_scores.csv\n",
                        "\n",
                        "New Marketing Breakdown:\n",
                        "mirror_verdict\n",
                        "Weak Mirror        2232\n",
                        "Moderate Mirror     631\n",
                        "Reject              578\n",
                        "Strong Mirror        48\n",
                        "Name: count, dtype: int64\n"
                    ]
                }
            ],
            "source": [
                "df['normalized_RS'] = (\n",
                "    (df['routine_score'] - df['routine_score'].min()) /\n",
                "    (df['routine_score'].max() - df['routine_score'].min())\n",
                ")\n",
                "\n",
                "df = df.sort_values(by='routine_score', ascending=False)\n",
                "df.to_csv(output_path, index=False)\n",
                "\n",
                "print(\"-\" * 50)\n",
                "print(f\"\\u2705 SUCCESS: Refined scores saved to {output_path}\")\n",
                "print(\"\\nNew Marketing Breakdown:\")\n",
                "print(df['mirror_verdict'].value_counts())"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.14.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
