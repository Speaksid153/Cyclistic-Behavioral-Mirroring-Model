{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Statistical Validation: Hourly Ride Distributions\n",
                "\n",
                "This notebook builds the foundational dataset for statistical validation. It aggregates hourly ride distributions for different station segments (Anchors vs. Noise), calculating the percentage share of daily rides for each hour. This data is essential for visualizing the 'Behavioral Peak' and validating the commuter signal at selected stations."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & Configuration\n",
                "\n",
                "Define paths for the fact table, station segments, and the validation output."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from pathlib import Path\n",
                "\n",
                "\n",
                "DATA_DIR = Path(\"../data/processed\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Statistical Ingestion & Transformation\n",
                "\n",
                "The validation process follows these steps:\n",
                "1. **Casual Filtering**: Isolates casual rides from the master fact table.\n",
                "2. **Segment Merging**: Integrates station classification labels (Anchors/Noise).\n",
                "3. **Hourly Aggregation**: Calculates the total rides per hour per segment.\n",
                "4. **Share Calculation**: Computes the percentage share of a segment's total daily volume for each specific hour, enabling a direct 'peakedness' comparison."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_statistical_validation():\n",
                "    master_path = DATA_DIR / \"fact_trips.csv\"\n",
                "    segment_path = DATA_DIR / \"station_behavior_segments.csv\"\n",
                "    output_path = DATA_DIR / \"hourly_validation_metrics.csv\"\n",
                "\n",
                "    if not master_path.exists() or not segment_path.exists():\n",
                "        print(\"❌ Error: Required datasets missing. Run pipeline and segmentation first.\")\n",
                "        return\n",
                "\n",
                "    print(\"Building Hourly Statistical Validation dataset...\")\n",
                "\n",
                "\n",
                "    df = pd.read_csv(master_path, usecols=['started_at', 'member_casual', 'start_station_name'])\n",
                "    df = df[df['member_casual'] == 'casual'].copy()\n",
                "    \n",
                "    segments = pd.read_csv(segment_path, usecols=['start_station_name', 'final_status'])\n",
                "\n",
                "   \n",
                "    df['hour'] = pd.to_datetime(df['started_at']).dt.hour\n",
                "    merged = df.merge(segments, on=\"start_station_name\", how=\"inner\")\n",
                "\n",
                "   \n",
                "    filtered = merged[merged[\"final_status\"].isin([\"Confirmed Behavioral Anchor\", \"Inconsistent / Noise\"])]\n",
                "\n",
                "   \n",
                "    hourly_dist = filtered.groupby([\"final_status\", \"hour\"]).size().reset_index(name=\"rides\")\n",
                "    \n",
                "    \n",
                "    hourly_dist[\"pct_of_daily_rides\"] = (\n",
                "        hourly_dist.groupby(\"final_status\")[\"rides\"]\n",
                "        .transform(lambda x: (x / x.sum()) * 100)\n",
                "    )\n",
                "\n",
                "  \n",
                "    hourly_dist.to_csv(output_path, index=False)\n",
                "    \n",
                "    print(\"-\" * 50)\n",
                "    print(f\"✅ SUCCESS: Validation metrics saved to {output_path}\")\n",
                "    print(\"This file will power your 'Behavioral Peak' Line Chart in Power BI.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Execution\n",
                "\n",
                "Execute the statistical validation pipeline."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Building Hourly Statistical Validation dataset...\n",
                        "--------------------------------------------------\n",
                        "✅ SUCCESS: Validation metrics saved to ..\\data\\processed\\hourly_validation_metrics.csv\n",
                        "This file will power your 'Behavioral Peak' Line Chart in Power BI.\n"
                    ]
                }
            ],
            "source": [
                "if __name__ == \"__main__\":\n",
                "    run_statistical_validation()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.14.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
