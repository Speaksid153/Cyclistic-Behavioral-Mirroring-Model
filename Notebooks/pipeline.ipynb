{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Analytics Pipeline: Data Engineering & Feature Extraction\n",
                "\n",
                "This notebook serves as the primary data processing engine for the Cyclistic Behavioral Mirroring Model. It performs batch processing of raw CSV files, handles data cleaning, and engineers critical binary features like `is_commute` to enable downstream behavioral modeling."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & Configuration\n",
                "\n",
                "Define directory paths for raw and processed data, and set stability thresholds for station analysis."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "from pathlib import Path\n",
                "RAW_DIR = Path(\"../data/raw\")\n",
                "PROCESSED_DIR = Path(\"../data/processed\")\n",
                "MIN_TRIPS_FOR_STABILITY = 50 "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Pipeline Orchestration\n",
                "\n",
                "The pipeline function executes the following sequence:\n",
                "1. **Ingestion**: Loads all CSV files from the raw directory.\n",
                "2. **Cleaning**: Standardizes column names, converts types, and filters out trips with null stations or invalid durations (e.g., < 1 min or > 24 hours).\n",
                "3. **Feature Engineering**: Derives `hour`, `is_weekday`, and the critical `is_commute` flag based on time-of-day and trip duration.\n",
                "4. **Export**: Saves a cleaned fact table (`fact_trips.csv`) and a station dimension table (`dim_stations.csv`) for subsequent analysis steps."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_analytics_pipeline():\n",
                "    PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
                "    \n",
                "    all_files = list(RAW_DIR.glob(\"*.csv\"))\n",
                "    if not all_files:\n",
                "        raise FileNotFoundError(f\"No CSVs found in {RAW_DIR}.\")\n",
                "\n",
                "   \n",
                "    keep_cols = [\n",
                "        'ride_id', 'started_at', 'ended_at', 'member_casual',\n",
                "        'start_station_name', 'start_lat', 'start_lng', 'end_lat', 'end_lng'\n",
                "    ]\n",
                "    \n",
                "    print(f\"Processing {len(all_files)} files...\")\n",
                "    df = pd.concat((pd.read_csv(f, usecols=lambda x: x.lower() in keep_cols) for f in all_files), ignore_index=True)\n",
                "    df.columns = df.columns.str.lower()\n",
                "\n",
                "    \n",
                "    df['started_at'] = pd.to_datetime(df['started_at'], errors='coerce')\n",
                "    df['ended_at'] = pd.to_datetime(df['ended_at'], errors='coerce')\n",
                "    df = df.dropna(subset=['started_at', 'start_station_name'])\n",
                "    \n",
                "    df['ride_length'] = (df['ended_at'] - df['started_at']).dt.total_seconds() / 60\n",
                "    df = df[(df['ride_length'] > 1) & (df['ride_length'] < 1440)]\n",
                "\n",
                "   \n",
                "    df['hour'] = df['started_at'].dt.hour\n",
                "    df['is_weekday'] = df['started_at'].dt.dayofweek < 5\n",
                "    is_rush = df['hour'].isin([7, 8, 9, 10, 16, 17, 18, 19])\n",
                "    df['is_commute'] = is_rush & df['is_weekday'] & (df['ride_length'] <= 30)\n",
                "\n",
                "   \n",
                "    station_dim = df.groupby('start_station_name').agg({\n",
                "        'start_lat': 'mean',\n",
                "        'start_lng': 'mean'\n",
                "    }).reset_index()\n",
                "    station_dim.to_csv(PROCESSED_DIR / \"dim_stations.csv\", index=False)\n",
                "\n",
                "    \n",
                "    fact_cols = [\n",
                "        'ride_id', 'start_station_name', 'started_at', 'member_casual', \n",
                "        'is_commute', 'ride_length', 'start_lat', 'start_lng', 'end_lat', 'end_lng'\n",
                "    ]\n",
                "    df[fact_cols].to_csv(PROCESSED_DIR / \"fact_trips.csv\", index=False)\n",
                "\n",
                "    print(f\"✅ SUCCESS: Pipeline re-run complete. Fact table now contains coordinates.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Execution\n",
                "\n",
                "Execute the end-to-end data pipeline."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Processing 13 files...\n",
                        "✅ SUCCESS: Pipeline re-run complete. Fact table now contains coordinates.\n"
                    ]
                }
            ],
            "source": [
                "if __name__ == \"__main__\":\n",
                "    run_analytics_pipeline()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.14.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
