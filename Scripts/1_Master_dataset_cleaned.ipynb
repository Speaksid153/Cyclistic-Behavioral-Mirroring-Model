{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Master Dataset Cleaning\n",
    "This notebook merges raw CSV datasets, standardizes columns, converts datetimes, and adds feature engineering columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# =========================\n",
    "# 1. PATH SETUP \n",
    "# =========================\n",
    "# This script is in 'DIVVY PROJECT/Scripts/'\n",
    "SCRIPT_DIR = Path().resolve()\n",
    "\n",
    "# Move up one level to 'DIVVY PROJECT/'\n",
    "# Note: In a notebook, we might need to adjust this depending on where the notebook is running.\n",
    "# Assuming the notebook is in the same 'Scripts' folder.\n",
    "PROJECT_ROOT = SCRIPT_DIR.parent \n",
    "\n",
    "# Define paths relative to the project root\n",
    "input_folder = PROJECT_ROOT / 'Data' / 'Raw Datasets'\n",
    "output_folder = PROJECT_ROOT / 'Data' / 'Processed Datasets'\n",
    "\n",
    "# Automatic folder creation (No more manual setup)\n",
    "output_folder.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 2. LOAD & MERGE CSV FILES\n",
    "# =========================\n",
    "# This finds all CSVs in the raw folder\n",
    "all_files = list(input_folder.glob(\"*.csv\"))\n",
    "\n",
    "if not all_files:\n",
    "    print(f\"âŒ ERROR: No CSV files found in: {input_folder}\")\n",
    "    print(\"Ensure your CSV files are inside the 'Data/Raw Datasets' folder.\")\n",
    "    # exit() # Commented out for notebook execution\n",
    "else:\n",
    "    print(f\"Found {len(all_files)} CSV files. Merging now (this may take a moment)...\")\n",
    "\n",
    "    # Efficiently load all CSVs\n",
    "    df_list = [pd.read_csv(file) for file in all_files]\n",
    "    df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 3. NORMALIZE COLUMN NAMES\n",
    "# =========================\n",
    "df.columns = df.columns.str.strip().str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 4. DATETIME CONVERSION\n",
    "# =========================\n",
    "print(\"Converting timestamps...\")\n",
    "df['started_at'] = pd.to_datetime(df['started_at'], errors='coerce')\n",
    "df['ended_at'] = pd.to_datetime(df['ended_at'], errors='coerce')\n",
    "\n",
    "# Drop rows with invalid timestamps\n",
    "df = df.dropna(subset=['started_at', 'ended_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 5. RIDE DURATION\n",
    "# =========================\n",
    "df['ride_length'] = (df['ended_at'] - df['started_at']).dt.total_seconds() / 60\n",
    "\n",
    "# Remove invalid durations (rides that ended before they started)\n",
    "df = df[df['ride_length'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 6. TIME-BASED FEATURES\n",
    "# =========================\n",
    "df['hour'] = df['started_at'].dt.hour\n",
    "df['day_of_week'] = df['started_at'].dt.day_name()\n",
    "df['month'] = df['started_at'].dt.month_name()\n",
    "df['month_num'] = df['started_at'].dt.month\n",
    "df['is_weekend'] = df['day_of_week'].isin(['Saturday', 'Sunday']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 7. RIDE LENGTH BUCKETS\n",
    "# =========================\n",
    "df['ride_length_bucket'] = pd.cut(\n",
    "    df['ride_length'],\n",
    "    bins=[0, 10, 30, 60, 120, 10000],\n",
    "    labels=['very_short', 'short', 'medium', 'long', 'very_long']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 8. BEHAVIORAL FLAGS\n",
    "# =========================\n",
    "# Commute-like rides: Weekdays, during rush hour, under 30 mins\n",
    "df['commuter_flag'] = (\n",
    "    (df['hour'].between(7, 10) | df['hour'].between(17, 20)) &\n",
    "    (df['is_weekend'] == 0) &\n",
    "    (df['ride_length'] <= 30)\n",
    ")\n",
    "\n",
    "# High conversion potential: Casual riders acting like commuters\n",
    "df['high_conversion_potential'] = (\n",
    "    (df['member_casual'] == 'casual') &\n",
    "    (df['commuter_flag'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 9. MASTER DATASET SAVING\n",
    "# =========================\n",
    "master_cols = [\n",
    "    'ride_id', 'rideable_type', 'started_at', 'ended_at',\n",
    "    'start_station_name', 'start_station_id', 'end_station_name', 'end_station_id',\n",
    "    'start_lat', 'start_lng', 'end_lat', 'end_lng',\n",
    "    'member_casual', 'ride_length', 'ride_length_bucket',\n",
    "    'hour', 'day_of_week', 'month', 'month_num',\n",
    "    'is_weekend', 'commuter_flag', 'high_conversion_potential'\n",
    "]\n",
    "\n",
    "# Ensure only existing columns are exported to avoid errors \n",
    "df_master = df.reindex(columns=master_cols)\n",
    "\n",
    "master_path = output_folder / 'cyclistic_master_dataset.csv'\n",
    "df_master.to_csv(master_path, index=False)\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\"SUCCESS: Master dataset saved to: {master_path}\")\n",
    "print(f\"Total Rows Processed: {len(df_master):,}\")\n",
    "print(\"Pipeline finished successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
